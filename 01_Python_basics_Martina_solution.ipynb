{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1cxx-GnWkMywOSX1k06xAQ9r-57lBenFW","timestamp":1730060475314}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/liadmagen/NLP-Course/blob/master/exercises_notebooks/01_LM_NLP_python_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"9k8FCi_UAtCN"},"source":["# Python Basics\n","\n","In this exercise, we'll explore python NLP capabilities with the help of the package `nltk`.\n","\n","By the end of the exercise, you will:\n","* be introduced to the nltk package and its functionality\n","* understand the basics of text analysis, and know how to approach this unstructured data.\n","* Understand the terms 'n-gram' & 'collocation'"]},{"cell_type":"markdown","metadata":{"id":"iF7evQxY31Kr"},"source":["We are going to use the package `NLTK` - 'Natural Language Toolkit' (https://www.nltk.org/).\n","\n","NLTK is a great package for research and for learning. However, it isn't recommended for production use and for real-world applications, as it isn't fast enough and therefore doesn't scale."]},{"cell_type":"markdown","metadata":{"id":"1OqX9GeN216c"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"wS2o9QMaUmzC"},"source":["import random\n","\n","import nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3gBd4ZgXzAmP","outputId":"440ad046-01f5-4625-f591-48f04c9fa780","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730105352987,"user_tz":-60,"elapsed":8164,"user":{"displayName":"Martina Kamodyová","userId":"12069066525446932918"}},"collapsed":true},"source":["nltk.download('book')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading collection 'book'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection book\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"LRzGNH63xcwQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730105364998,"user_tz":-60,"elapsed":5797,"user":{"displayName":"Martina Kamodyová","userId":"12069066525446932918"}},"outputId":"d7715368-96ad-47e4-d732-f71cf5fe58f6","collapsed":true},"source":["from nltk.book import *"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["*** Introductory Examples for the NLTK Book ***\n","Loading text1, ..., text9 and sent1, ..., sent9\n","Type the name of the text or sentence to view it.\n","Type: 'texts()' or 'sents()' to list the materials.\n","text1: Moby Dick by Herman Melville 1851\n","text2: Sense and Sensibility by Jane Austen 1811\n","text3: The Book of Genesis\n","text4: Inaugural Address Corpus\n","text5: Chat Corpus\n","text6: Monty Python and the Holy Grail\n","text7: Wall Street Journal\n","text8: Personals Corpus\n","text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"]}]},{"cell_type":"markdown","source":["# Exploratory Data Analysis (EDA)"],"metadata":{"id":"J_dcD1AYhBez"}},{"cell_type":"markdown","metadata":{"id":"FlR3eNPu3EWS"},"source":["## A Closer Look at Python: Texts as Lists of Words"]},{"cell_type":"markdown","metadata":{"id":"Ul9GAlK63sMj"},"source":["We will use the great book 'Moby Dick' by Herman Melville, as our learning experiment playground.\n","\n","The book is already tokenized and stored as a list of these tokens, under a variable with the excellent, well expressed name - `text1` (please do yourself - and me - a favor and name your variables in a more meaningful manner than that...).\n","\n","We start - as you should always do - with exploring and looking at our dataset.\n","\n","Let's peek at the first 100 words:"]},{"cell_type":"code","metadata":{"id":"jjIvrcow3Pjc","outputId":"3af46384-4278-4c78-dc03-f0f8e7c4f0c5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730105368811,"user_tz":-60,"elapsed":332,"user":{"displayName":"Martina Kamodyová","userId":"12069066525446932918"}},"collapsed":true},"source":["text1[:100]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[',\n"," 'Moby',\n"," 'Dick',\n"," 'by',\n"," 'Herman',\n"," 'Melville',\n"," '1851',\n"," ']',\n"," 'ETYMOLOGY',\n"," '.',\n"," '(',\n"," 'Supplied',\n"," 'by',\n"," 'a',\n"," 'Late',\n"," 'Consumptive',\n"," 'Usher',\n"," 'to',\n"," 'a',\n"," 'Grammar',\n"," 'School',\n"," ')',\n"," 'The',\n"," 'pale',\n"," 'Usher',\n"," '--',\n"," 'threadbare',\n"," 'in',\n"," 'coat',\n"," ',',\n"," 'heart',\n"," ',',\n"," 'body',\n"," ',',\n"," 'and',\n"," 'brain',\n"," ';',\n"," 'I',\n"," 'see',\n"," 'him',\n"," 'now',\n"," '.',\n"," 'He',\n"," 'was',\n"," 'ever',\n"," 'dusting',\n"," 'his',\n"," 'old',\n"," 'lexicons',\n"," 'and',\n"," 'grammars',\n"," ',',\n"," 'with',\n"," 'a',\n"," 'queer',\n"," 'handkerchief',\n"," ',',\n"," 'mockingly',\n"," 'embellished',\n"," 'with',\n"," 'all',\n"," 'the',\n"," 'gay',\n"," 'flags',\n"," 'of',\n"," 'all',\n"," 'the',\n"," 'known',\n"," 'nations',\n"," 'of',\n"," 'the',\n"," 'world',\n"," '.',\n"," 'He',\n"," 'loved',\n"," 'to',\n"," 'dust',\n"," 'his',\n"," 'old',\n"," 'grammars',\n"," ';',\n"," 'it',\n"," 'somehow',\n"," 'mildly',\n"," 'reminded',\n"," 'him',\n"," 'of',\n"," 'his',\n"," 'mortality',\n"," '.',\n"," '\"',\n"," 'While',\n"," 'you',\n"," 'take',\n"," 'in',\n"," 'hand',\n"," 'to',\n"," 'school',\n"," 'others',\n"," ',']"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"gD5ADOnC48oM"},"source":["Pay attention that punctuations here are also conisdered as a `token`."]},{"cell_type":"markdown","metadata":{"id":"uthWKmm153r-"},"source":["## Exercise #1: Show the last 23 tokens in the book:"]},{"cell_type":"code","metadata":{"id":"SvWRtYl-3Ubi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730105372868,"user_tz":-60,"elapsed":252,"user":{"displayName":"Martina Kamodyová","userId":"12069066525446932918"}},"outputId":"57fdc68e-f65e-440f-d7cd-f25af165654d"},"source":["### YOUR TURN:\n","### Write a code that shows the last sentence (23 tokens) of the book\n","\n","## this is when we know that the last sentence has 23 tokens\n","print(text1[-23:])\n","\n","## if we didn't know that than:\n","text_string = \" \".join(text1)\n","# Tokenize the string into sentences using NLTK\n","sentences = nltk.sent_tokenize(text_string)\n","print(sentences[-1])\n","\n","### End"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['It', 'was', 'the', 'devious', '-', 'cruising', 'Rachel', ',', 'that', 'in', 'her', 'retracing', 'search', 'after', 'her', 'missing', 'children', ',', 'only', 'found', 'another', 'orphan', '.']\n","It was the devious - cruising Rachel , that in her retracing search after her missing children , only found another orphan .\n"]}]},{"cell_type":"markdown","metadata":{"id":"sC0w4C5T68wF"},"source":["## Lists vs Sets\n","\n","In python, an ordered set, with repetition, is defined as a `List`, and is defied by sqaured braces [].\n","\n","An unordered set, where repetitions are *discarded*, is defined with curly braces: {}.\n","\n","When converting a list into a set, we can get the **vocabulary** of the corpus, the *unique* words that the dataset is constructed of:"]},{"cell_type":"code","metadata":{"id":"MsUBYcwl6Cdc","outputId":"6bed2e75-4835-485e-8e24-a342536855f9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730105379612,"user_tz":-60,"elapsed":296,"user":{"displayName":"Martina Kamodyová","userId":"12069066525446932918"}},"collapsed":true},"source":["vocab = set(text1)\n","\n","# We can't get the 'last 25 words', since there is no order...\n","# But we can convert it into a list first, and even sort it\n","list(sorted(vocab))[-50:]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['yawned',\n"," 'yawning',\n"," 'ye',\n"," 'yea',\n"," 'year',\n"," 'yearly',\n"," 'years',\n"," 'yeast',\n"," 'yell',\n"," 'yelled',\n"," 'yelling',\n"," 'yellow',\n"," 'yellowish',\n"," 'yells',\n"," 'yes',\n"," 'yesterday',\n"," 'yet',\n"," 'yield',\n"," 'yielded',\n"," 'yielding',\n"," 'yields',\n"," 'yoke',\n"," 'yoked',\n"," 'yokes',\n"," 'yoking',\n"," 'yon',\n"," 'yonder',\n"," 'yore',\n"," 'you',\n"," 'young',\n"," 'younger',\n"," 'youngest',\n"," 'youngish',\n"," 'your',\n"," 'yours',\n"," 'yourselbs',\n"," 'yourself',\n"," 'yourselves',\n"," 'youth',\n"," 'youthful',\n"," 'zag',\n"," 'zay',\n"," 'zeal',\n"," 'zephyr',\n"," 'zig',\n"," 'zodiac',\n"," 'zone',\n"," 'zoned',\n"," 'zones',\n"," 'zoology']"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"LkvijxKv8nqm"},"source":["## Exercise #2: Vocabulary Length\n","\n","How many words does our vocabulary contain?"]},{"cell_type":"code","metadata":{"id":"XJnFJcsG8k2X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730105383516,"user_tz":-60,"elapsed":298,"user":{"displayName":"Martina Kamodyová","userId":"12069066525446932918"}},"outputId":"994720b6-a2c6-4cc4-9bad-684b2801b32a"},"source":["### YOUR TURN:\n","### Write python code that prints the size of Moby Dick book's vocabulary\n","\n","print(len(text1))\n","\n","### End"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["260819\n"]}]},{"cell_type":"markdown","metadata":{"id":"QxlwVLXG9BRg"},"source":["# Text Analysis: Frequency Distribution"]},{"cell_type":"markdown","metadata":{"id":"IlpuCzpK9H9J"},"source":["[nltk](http://www.nltk.org) is a library with many research tools for probabilistic information and dataset exploration.\n","\n","For example, it includes a function, `FreqDist`, that return the probability of the occurance of a word in a text:\n","\n","http://www.nltk.org/api/nltk.html?highlight=freqdist#module-nltk.probability"]},{"cell_type":"code","metadata":{"id":"z76z5LY19FQi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730105386806,"user_tz":-60,"elapsed":389,"user":{"displayName":"Martina Kamodyová","userId":"12069066525446932918"}},"outputId":"991a96c1-6766-448d-c3c8-6885889cc61b"},"source":["### YOUR TURN:\n","## 1) Write python function named `get_most_frequent(n: int)` that calculates the frequency of words in text1 and returns the top n common ones (n is given as a parameter).\n","## 2) Write a python function - `get_frequency(words: list[str])` that given a list of words, prints the frequency of each of those words in text1.\n","## 3) Use the functions to print how many times the words 'with', 'Moby', 'fish' and 'whale' appear in the book.\n","## hint: FreqDist is a smart python dictionary that already has methods for these tasks, such as .most_common()\n","\n","from nltk import FreqDist\n","\n","# creates a frequency distribution from the text1\n","frequency_distribution = FreqDist(text1)\n","\n","# Task 1: Function to get the top n most frequent words\n","def get_most_frequent(n: int):\n","  # Return the top n most common words and their counts\n","    return frequency_distribution.most_common(n)\n","\n","# Task 2: Function to print frequency of specific words\n","def get_frequency(words: list[str]):\n","    for word in words:\n","        print(f\"'{word}' appears {frequency_distribution[word]} times in the text.\")\n","\n","### End\n","\n","get_frequency([\"with\", \"Moby\", \"fish\", \"whale\"])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'with' appears 1659 times in the text.\n","'Moby' appears 84 times in the text.\n","'fish' appears 133 times in the text.\n","'whale' appears 906 times in the text.\n"]}]},{"cell_type":"code","source":["assert get_most_frequent(5) == [(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024)]"],"metadata":{"id":"ttVE805EfZP3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F39jB_1jAboe"},"source":[":Some of the common words are actually punctuations and '**stop-words**'. They don't help us much with our text analysis, and therefore can be safely ignored.\n","\n","Luckily, NLTK supplies a list of stop words, and python has the punctuation built in into the string package:"]},{"cell_type":"code","metadata":{"id":"Qne74woEATJo","outputId":"563e8f72-b305-4d72-ad83-2b83949b92be","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730105392480,"user_tz":-60,"elapsed":312,"user":{"displayName":"Martina Kamodyová","userId":"12069066525446932918"}}},"source":["from nltk.corpus import stopwords\n","\n","print(stopwords.words('english'))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"]}]},{"cell_type":"code","metadata":{"id":"8NWLKWf79z94","outputId":"474c667a-35b8-4776-e038-1214dcd86923","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730105393879,"user_tz":-60,"elapsed":3,"user":{"displayName":"Martina Kamodyová","userId":"12069066525446932918"}}},"source":["import string\n","\n","print(string.punctuation)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"]}]},{"cell_type":"code","metadata":{"id":"APuonB9pA1T3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730105396759,"user_tz":-60,"elapsed":316,"user":{"displayName":"Martina Kamodyová","userId":"12069066525446932918"}},"outputId":"c42c3e29-9943-4e0d-ced0-c82544059b89"},"source":["### Write a function - get_most_frequent_filtered(n: int) - that returns the top\n","### n frequennt words, after filtering out stop words and punctuation.\n","\n","\n","def get_most_frequent_filtered(n: int):\n","    # Get the English stop words from NLTK\n","    stop_words = set(stopwords.words('english'))\n","\n","    # Create a set of punctuation characters from the string module\n","    punctuation = set(string.punctuation)\n","    #the character \"--\" is not detected as punctuation; when running my code, this\n","    #  character appears as the most frequent word (punctuation already filtered) and so the assert evaluates as False;\n","    #  so I simply added this string to the punctuation set in order to omit it when calling get_most_frequent_filtered\n","    punctuation.add(\"--\")\n","\n","    # Filter out stop words and punctuation from text1\n","    filtered_words = []\n","    for word in text1:\n","        if word.lower() not in stop_words and word not in punctuation:\n","            filtered_words.append(word)\n","\n","    # Create a frequency distribution from the filtered words\n","    frequency_distribution_filtered = FreqDist(filtered_words)\n","\n","    # Return only the top n most common words after filtering\n","    return frequency_distribution_filtered.most_common(n)\n","\n","###\n","\n","print(get_most_frequent_filtered(5))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('whale', 906), ('one', 889), ('like', 624), ('upon', 538), ('man', 508)]\n"]}]},{"cell_type":"code","source":["assert get_most_frequent_filtered(5) == [('whale', 906), ('one', 889), ('like', 624), ('upon', 538), ('man', 508)]"],"metadata":{"id":"Y17Fx8baf68k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wGCb2GiEEuqM"},"source":["FreqDist can be used even further. Let's analyse the text by the word length.\n","\n","Using python ['list-comprehension'](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions) method, we can easily get a list of all the words by their lengths:"]},{"cell_type":"code","metadata":{"id":"QBYSet5WEtcS","outputId":"5765adfb-ca6b-4f45-95e3-46e8ca979436","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730105403090,"user_tz":-60,"elapsed":428,"user":{"displayName":"Martina Kamodyová","userId":"12069066525446932918"}},"collapsed":true},"source":["# For convenience of reading, showing here only the first 30\n","[len(w) for w in text1][:30]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1,\n"," 4,\n"," 4,\n"," 2,\n"," 6,\n"," 8,\n"," 4,\n"," 1,\n"," 9,\n"," 1,\n"," 1,\n"," 8,\n"," 2,\n"," 1,\n"," 4,\n"," 11,\n"," 5,\n"," 2,\n"," 1,\n"," 7,\n"," 6,\n"," 1,\n"," 3,\n"," 4,\n"," 5,\n"," 2,\n"," 10,\n"," 2,\n"," 4,\n"," 1]"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["## Exercise #3 (Advanced): Length Frequency"],"metadata":{"id":"gK9wjxkQbP2I"}},{"cell_type":"code","metadata":{"id":"BgHoCYA-FXDa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730105406404,"user_tz":-60,"elapsed":597,"user":{"displayName":"Martina Kamodyová","userId":"12069066525446932918"}},"outputId":"b6f575e1-0bbc-4d9f-b14d-5661ea46bc59","collapsed":true},"source":["### Write a code to calculate the words lengthes frequency inside `text1`.\n","### Find out what those 20 words are.\n","### How many times do the 20 most lengthiest words appear in the text?\n","\n","\n","### CALCULATE THE WORDS LENGTHS FREQUENCY INSIDE text1:\n","# our text1 is already tokenized (all words, incl. punctuation, are tokens)\n","\n","# Remove punctuation from the words --> removing punctuation by keeping only alphanumeric tokens\n","words_filtered = [word for word in text1 if word.isalnum()]\n","\n","# Calculate the length of each word\n","word_lengths = [len(word) for word in words_filtered]\n","\n","# Create a frequency distribution of word lengths --> tells you how many words of each length are present in your text\n","length_frequency = FreqDist(word_lengths)\n","\n","\n","\n","### 20 LONGEST WORDS (in a descending order)\n","# Convert unique words to a list and sort it by length\n","unique_words = list(words_filtered)\n","sorted_words_by_length = sorted(unique_words, key=len, reverse=True)\n","\n","# Find the 20 longest words\n","longest_words = sorted_words_by_length[:20]\n","\n","\n","\n","### COUNT THE OCCURENCES OF THE 20 LONGEST WORDS IN THE TEXT\n","# Count the occurrences of the 20 longest words in the text - creating a dictionary\n","#   Each key is a word from longest_words.\n","#   Each value is the count of how many times that word appears in words_filtered using the count() method.\n","# comprehensed: longest_word_counts = {word: words_filtered.count(word) for word in longest_words}\n","longest_word_counts = {}\n","for word in longest_words:\n","    longest_word_counts[word] = words_filtered.count(word)\n","\n","# Preparing the longest_words_output-string --> for printing each key-value pair in one line\n","#comprehensed: longest_words_output = \"\\n\".join([f\"{word}: {count}\" for word, count in longest_word_counts.items()])\n","longest_words_output = \"\"\n","for word, count in longest_word_counts.items():\n","    longest_words_output += f\"{word}: {count}\\n\"\n","\n","\n","\n","print(\"Word Length Frequency Distribution:\", length_frequency)\n","\n","print(\"20 Longest Words:\", longest_words)\n","# printing each of the 20 longest words in one line:\n","print(\"20 Longest Words:\\n\" + \"\\n\".join(longest_words))\n","\n","print(\"Total Appearances of the 20 Longest Words:\", longest_word_counts)\n","# printing the dictionary with each key-value pair in one line:\n","print(\"Total Appearances of the 20 Longest Words:\\n\", longest_words_output)\n","\n","### End"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Word Length Frequency Distribution: <FreqDist with 19 samples and 218619 outcomes>\n","20 Longest Words: ['uninterpenetratingly', 'characteristically', 'uncomfortableness', 'cannibalistically', 'circumnavigations', 'superstitiousness', 'superstitiousness', 'comprehensiveness', 'preternaturalness', 'indispensableness', 'comprehensiveness', 'comprehensiveness', 'uncompromisedness', 'subterraneousness', 'CIRCUMNAVIGATION', 'apprehensiveness', 'indiscriminately', 'indiscriminately', 'circumnavigating', 'circumnavigation']\n","20 Longest Words:\n","uninterpenetratingly\n","characteristically\n","uncomfortableness\n","cannibalistically\n","circumnavigations\n","superstitiousness\n","superstitiousness\n","comprehensiveness\n","preternaturalness\n","indispensableness\n","comprehensiveness\n","comprehensiveness\n","uncompromisedness\n","subterraneousness\n","CIRCUMNAVIGATION\n","apprehensiveness\n","indiscriminately\n","indiscriminately\n","circumnavigating\n","circumnavigation\n","Total Appearances of the 20 Longest Words: {'uninterpenetratingly': 1, 'characteristically': 1, 'uncomfortableness': 1, 'cannibalistically': 1, 'circumnavigations': 1, 'superstitiousness': 2, 'comprehensiveness': 3, 'preternaturalness': 1, 'indispensableness': 1, 'uncompromisedness': 1, 'subterraneousness': 1, 'CIRCUMNAVIGATION': 1, 'apprehensiveness': 4, 'indiscriminately': 3, 'circumnavigating': 2, 'circumnavigation': 2}\n","Total Appearances of the 20 Longest Words:\n"," uninterpenetratingly: 1\n","characteristically: 1\n","uncomfortableness: 1\n","cannibalistically: 1\n","circumnavigations: 1\n","superstitiousness: 2\n","comprehensiveness: 3\n","preternaturalness: 1\n","indispensableness: 1\n","uncompromisedness: 1\n","subterraneousness: 1\n","CIRCUMNAVIGATION: 1\n","apprehensiveness: 4\n","indiscriminately: 3\n","circumnavigating: 2\n","circumnavigation: 2\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"Buil9-Z-Emvk"},"source":["# Text Analysis: n-grams and collocation"]},{"cell_type":"markdown","metadata":{"id":"fVKEjI4GCXei"},"source":["As we saw in class, a word might not always also be a `token`. In the case of 'New York', 'ice cream', 'red wine', etc., every word meaning on its own is different than the combined meaning as a phrase.\n","\n","A **collocation** is a sequence of words that occur together unusually often.\n","\n","An `n-gram` is a sequence of a size of 'n' of tokens (i.e. words):\n","\n","* When n=1: it is called **unigram**\n","* When n=2: it is called **bigram**\n","* When n=3: it is called **trigram** ...\n","* When n>3: it is just called an **n-gram** with the size of 4.\n","\n","\n","NLTK has two functions: `bigrams` and `collocations`"]},{"cell_type":"code","metadata":{"id":"itg3dYQ3DeSG","outputId":"4b4bbc8f-1df9-44f0-b3c9-bfd34778d659","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730105414212,"user_tz":-60,"elapsed":325,"user":{"displayName":"Martina Kamodyová","userId":"12069066525446932918"}}},"source":["list(bigrams([1,2,3,4,5]))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1, 2), (2, 3), (3, 4), (4, 5)]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"HosjcKYuEIac"},"source":["## Bigrams generates bi-grams from the text: every two words would be collected together.\n","list(bigrams(text1))[:20]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PUqPHOytDfBx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730105444399,"user_tz":-60,"elapsed":258,"user":{"displayName":"Martina Kamodyová","userId":"12069066525446932918"}},"outputId":"53c7891e-c4b8-4f21-cb77-77e0d0e65d5a"},"source":["### Your Turn ###\n","# Write here code that returns and print the collocations in text1\n","\n","from nltk import ngrams\n","from collections import Counter\n","\n","### MY CODE WHICH PROVIDED ALSO SOME UNWANTED COLLOCATIONS IN THE OUTPUT:\n","# Define stop words and punctuation\n","stop_words = set(stopwords.words('english'))\n","punctuation = set(string.punctuation)\n","\n","# Filter out stop words and punctuation\n","filtered_words = [word for word in text1 if word.lower() not in stop_words and word not in punctuation]\n","\n","# Remove any remaining unwanted characters\n","filtered_words = [word for word in filtered_words if word.isalpha()]\n","\n","# Create bigrams\n","bigrams = ngrams(filtered_words, 2)\n","\n","# Count the frequency of each bigram\n","bigram_counts = Counter(bigrams)\n","\n","# Get the most common bigrams\n","most_common_bigrams = bigram_counts.most_common(20)\n","\n","# Prepare the output format\n","collocations = [\" \".join(bigram) for bigram, count in most_common_bigrams]\n","\n","# Print the collocations\n","print(\"; \".join(collocations))\n","\n","\n","### A SUGGESTION FROM MY COLLEAUGES - a method that could replace my whole code:\n","text1.collocations()\n","\n","\n","# Expected output:\n","# Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; sperm\n","# whale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;\n","# years ago; lower jaw; never mind; Father Mapple; cried Stubb; chief\n","# mate; white whale; ivory leg; one hand\n","\n","### END ###########"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; sperm\n","whale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;\n","years ago; lower jaw; never mind; Father Mapple; cried Stubb; chief\n","mate; white whale; ivory leg; one hand\n"]}]},{"cell_type":"markdown","metadata":{"id":"zko0TVbLHWbl"},"source":["# Python and NLP"]},{"cell_type":"markdown","metadata":{"id":"T2vqtYKYHaPa"},"source":["Python has many strong capabilities, built in, when it comes to string and text procesing, combined with the list comprehension.\n","\n","Here are some examples of filtering the word list:"]},{"cell_type":"code","metadata":{"id":"FWs-o3xoHZck","outputId":"41d5a68d-0681-42af-b786-b67d13f6a63e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730061836833,"user_tz":-60,"elapsed":445,"user":{"displayName":"Martina Kamodyová","userId":"12069066525446932918"}}},"source":["# Get all the words that ends with 'ableness', sorted:\n","sorted(w for w in set(text1) if w.endswith('ableness'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['comfortableness',\n"," 'honourableness',\n"," 'immutableness',\n"," 'indispensableness',\n"," 'indomitableness',\n"," 'intolerableness',\n"," 'palpableness',\n"," 'reasonableness',\n"," 'uncomfortableness']"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"I1Dp1G3nHv3N","outputId":"12db8e73-c2cf-4e6a-dc05-13c430a206fa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730061839652,"user_tz":-60,"elapsed":718,"user":{"displayName":"Martina Kamodyová","userId":"12069066525446932918"}}},"source":["# Get all the words that contains 'orate', sorted:\n","sorted(term for term in set(text1) if 'orate' in term)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['camphorated',\n"," 'corroborated',\n"," 'decorated',\n"," 'elaborate',\n"," 'elaborately',\n"," 'evaporate',\n"," 'evaporates',\n"," 'incorporate',\n"," 'incorporated']"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"YxVjcOg_H1e8","outputId":"e36feca2-3e55-425e-b4ee-72f160482a71","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1730061843106,"user_tz":-60,"elapsed":1008,"user":{"displayName":"Martina Kamodyová","userId":"12069066525446932918"}}},"source":["# Get all the words which their first letter is capitalized:\n","sorted(item for item in set(text1) if item.istitle())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['3D',\n"," 'A',\n"," 'Abashed',\n"," 'Abednego',\n"," 'Abel',\n"," 'Abjectus',\n"," 'Aboard',\n"," 'Abominable',\n"," 'About',\n"," 'Above',\n"," 'Abraham',\n"," 'Academy',\n"," 'Accessory',\n"," 'According',\n"," 'Accordingly',\n"," 'Accursed',\n"," 'Achilles',\n"," 'Actium',\n"," 'Acushnet',\n"," 'Adam',\n"," 'Adieu',\n"," 'Adios',\n"," 'Admiral',\n"," 'Admirals',\n"," 'Advance',\n"," 'Advancement',\n"," 'Adventures',\n"," 'Adverse',\n"," 'Advocate',\n"," 'Affected',\n"," 'Affidavit',\n"," 'Affrighted',\n"," 'Afric',\n"," 'Africa',\n"," 'African',\n"," 'Africans',\n"," 'Aft',\n"," 'After',\n"," 'Afterwards',\n"," 'Again',\n"," 'Against',\n"," 'Agassiz',\n"," 'Ages',\n"," 'Ah',\n"," 'Ahab',\n"," 'Ahabs',\n"," 'Ahasuerus',\n"," 'Ahaz',\n"," 'Ahoy',\n"," 'Ain',\n"," 'Air',\n"," 'Akin',\n"," 'Alabama',\n"," 'Aladdin',\n"," 'Alarmed',\n"," 'Alas',\n"," 'Albatross',\n"," 'Albemarle',\n"," 'Albert',\n"," 'Albicore',\n"," 'Albino',\n"," 'Aldrovandi',\n"," 'Aldrovandus',\n"," 'Alexander',\n"," 'Alexanders',\n"," 'Alfred',\n"," 'Algerine',\n"," 'Algiers',\n"," 'Alike',\n"," 'Alive',\n"," 'All',\n"," 'Alleghanian',\n"," 'Alleghanies',\n"," 'Alley',\n"," 'Almanack',\n"," 'Almighty',\n"," 'Almost',\n"," 'Aloft',\n"," 'Alone',\n"," 'Alps',\n"," 'Already',\n"," 'Also',\n"," 'Am',\n"," 'Ambergriese',\n"," 'Ambergris',\n"," 'Amelia',\n"," 'America',\n"," 'American',\n"," 'Americans',\n"," 'Americas',\n"," 'Amittai',\n"," 'Among',\n"," 'Amsterdam',\n"," 'An',\n"," 'Anacharsis',\n"," 'Anak',\n"," 'Anatomist',\n"," 'And',\n"," 'Andes',\n"," 'Andrew',\n"," 'Andromeda',\n"," 'Angel',\n"," 'Angelo',\n"," 'Angels',\n"," 'Animated',\n"," 'Annawon',\n"," 'Anne',\n"," 'Anno',\n"," 'Anomalous',\n"," 'Another',\n"," 'Answer',\n"," 'Antarctic',\n"," 'Antilles',\n"," 'Antiochus',\n"," 'Antony',\n"," 'Antwerp',\n"," 'Anvil',\n"," 'Any',\n"," 'Anyhow',\n"," 'Anything',\n"," 'Anyway',\n"," 'Apollo',\n"," 'Apoplexy',\n"," 'Applied',\n"," 'Apply',\n"," 'April',\n"," 'Aquarius',\n"," 'Arch',\n"," 'Archbishop',\n"," 'Arched',\n"," 'Archer',\n"," 'Archipelagoes',\n"," 'Archy',\n"," 'Arctic',\n"," 'Are',\n"," 'Arethusa',\n"," 'Argo',\n"," 'Aries',\n"," 'Arion',\n"," 'Aristotle',\n"," 'Ark',\n"," 'Arkansas',\n"," 'Arkite',\n"," 'Arm',\n"," 'Armada',\n"," 'Arnold',\n"," 'Aroostook',\n"," 'Around',\n"," 'Arrayed',\n"," 'Arrived',\n"," 'Arsacidean',\n"," 'Arsacides',\n"," 'Art',\n"," 'Artedi',\n"," 'Arter',\n"," 'Articles',\n"," 'As',\n"," 'Asa',\n"," 'Ashantee',\n"," 'Ashore',\n"," 'Asia',\n"," 'Asiatic',\n"," 'Asiatics',\n"," 'Aside',\n"," 'Asphaltites',\n"," 'Assaulted',\n"," 'Assume',\n"," 'Assuming',\n"," 'Assuredly',\n"," 'Assyrian',\n"," 'Astern',\n"," 'Astir',\n"," 'Astronomy',\n"," 'At',\n"," 'Atlantic',\n"," 'Atlantics',\n"," 'Attached',\n"," 'Attend',\n"," 'August',\n"," 'Aunt',\n"," 'Australia',\n"," 'Australian',\n"," 'Austrian',\n"," 'Author',\n"," 'Authors',\n"," 'Auto',\n"," 'Availing',\n"," 'Avast',\n"," 'Avatar',\n"," 'Aware',\n"," 'Away',\n"," 'Awful',\n"," 'Ay',\n"," 'Aye',\n"," 'Azores',\n"," 'Babel',\n"," 'Babylon',\n"," 'Babylonian',\n"," 'Bachelor',\n"," 'Back',\n"," 'Backs',\n"," 'Bad',\n"," 'Baden',\n"," 'Bag',\n"," 'Balaene',\n"," 'Baliene',\n"," 'Baling',\n"," 'Bally',\n"," 'Baltic',\n"," 'Baltimore',\n"," 'Bamboo',\n"," 'Bang',\n"," 'Banks',\n"," 'Barbary',\n"," 'Bare',\n"," 'Bargain',\n"," 'Baron',\n"," 'Barrens',\n"," 'Bartholomew',\n"," 'Base',\n"," 'Bashaw',\n"," 'Bashee',\n"," 'Basilosaurus',\n"," 'Bastille',\n"," 'Battering',\n"," 'Battery',\n"," 'Bay',\n"," 'Bays',\n"," 'Be',\n"," 'Beach',\n"," 'Beale',\n"," 'Beams',\n"," 'Bear',\n"," 'Bears',\n"," 'Beat',\n"," 'Because',\n"," 'Becket',\n"," 'Bedford',\n"," 'Beelzebub',\n"," 'Befooled',\n"," 'Before',\n"," 'Begone',\n"," 'Behold',\n"," 'Behring',\n"," 'Being',\n"," 'Belated',\n"," 'Belial',\n"," 'Believe',\n"," 'Belisarius',\n"," 'Bell',\n"," 'Bellies',\n"," 'Beloved',\n"," 'Below',\n"," 'Belshazzar',\n"," 'Belubed',\n"," 'Bench',\n"," 'Bendigoes',\n"," 'Beneath',\n"," 'Bengal',\n"," 'Benjamin',\n"," 'Bennett',\n"," 'Bentham',\n"," 'Berkshire',\n"," 'Berlin',\n"," 'Bernard',\n"," 'Besides',\n"," 'Bess',\n"," 'Best',\n"," 'Bestow',\n"," 'Bethink',\n"," 'Better',\n"," 'Betty',\n"," 'Between',\n"," 'Beware',\n"," 'Beyond',\n"," 'Bible',\n"," 'Bibles',\n"," 'Bibliographical',\n"," 'Bildad',\n"," 'Biographical',\n"," 'Birmah',\n"," 'Bishop',\n"," 'Bite',\n"," 'Black',\n"," 'Blacksmith',\n"," 'Blackstone',\n"," 'Blanc',\n"," 'Blanche',\n"," 'Blanco',\n"," 'Blang',\n"," 'Blanket',\n"," 'Blast',\n"," 'Bless',\n"," 'Blind',\n"," 'Blinding',\n"," 'Blocksburg',\n"," 'Blood',\n"," 'Bloody',\n"," 'Blue',\n"," 'Boat',\n"," 'Boats',\n"," 'Bobbing',\n"," 'Bolivia',\n"," 'Bombay',\n"," 'Bonapartes',\n"," 'Bone',\n"," 'Bones',\n"," 'Bonneterre',\n"," 'Booble',\n"," 'Book',\n"," 'Boomer',\n"," 'Boone',\n"," 'Bordeaux',\n"," 'Borean',\n"," 'Born',\n"," 'Borneo',\n"," 'Bosom',\n"," 'Boston',\n"," 'Both',\n"," 'Bottle',\n"," 'Bottom',\n"," 'Bourbons',\n"," 'Bout',\n"," 'Bouton',\n"," 'Bowditch',\n"," 'Bower',\n"," 'Boy',\n"," 'Boys',\n"," 'Brace',\n"," 'Brahma',\n"," 'Brahmins',\n"," 'Brandreth',\n"," 'Brazil',\n"," 'Breakfast',\n"," 'Bremen',\n"," 'Bress',\n"," 'Bridge',\n"," 'Brighggians',\n"," 'Bright',\n"," 'Bring',\n"," 'Brisson',\n"," 'Brit',\n"," 'Britain',\n"," 'British',\n"," 'Britons',\n"," 'Broad',\n"," 'Broadway',\n"," 'Broke',\n"," 'Brother',\n"," 'Browne',\n"," 'Brute',\n"," 'Buckets',\n"," 'Bud',\n"," 'Buffalo',\n"," 'Bulkington',\n"," 'Bull',\n"," 'Bulwarks',\n"," 'Bunger',\n"," 'Bungle',\n"," 'Bunyan',\n"," 'Buoy',\n"," 'Buoyed',\n"," 'Burke',\n"," 'Burkes',\n"," 'Burst',\n"," 'Burton',\n"," 'Burtons',\n"," 'Business',\n"," 'But',\n"," 'Butchers',\n"," 'Butler',\n"," 'By',\n"," 'Byward',\n"," 'C',\n"," 'Cabaco',\n"," 'Cabin',\n"," 'Cachalot',\n"," 'Cadiz',\n"," 'Caesar',\n"," 'Caesarian',\n"," 'Cain',\n"," 'Calais',\n"," 'Californian',\n"," 'Call',\n"," 'Callao',\n"," 'Cambyses',\n"," 'Camel',\n"," 'Campagna',\n"," 'Can',\n"," 'Canaan',\n"," 'Canada',\n"," 'Canadian',\n"," 'Canal',\n"," 'Canaller',\n"," 'Canallers',\n"," 'Canals',\n"," 'Canaris',\n"," 'Cancer',\n"," 'Candles',\n"," 'Cannibal',\n"," 'Cannibals',\n"," 'Cannon',\n"," 'Canst',\n"," 'Cant',\n"," 'Canterbury',\n"," 'Cap',\n"," 'Cape',\n"," 'Capes',\n"," 'Capricornus',\n"," 'Captain',\n"," 'Captains',\n"," 'Capting',\n"," 'Caramba',\n"," 'Careful',\n"," 'Carefully',\n"," 'Carey',\n"," 'Carpenter',\n"," 'Carpet',\n"," 'Carrol',\n"," 'Carson',\n"," 'Carthage',\n"," 'Caryatid',\n"," 'Case',\n"," 'Cash',\n"," 'Cassock',\n"," 'Castaway',\n"," 'Castle',\n"," 'Categut',\n"," 'Cathedral',\n"," 'Catholic',\n"," 'Cato',\n"," 'Catskill',\n"," 'Cattegat',\n"," 'Caught',\n"," 'Cave',\n"," 'Caw',\n"," 'Cellini',\n"," 'Central',\n"," 'Certain',\n"," 'Certainly',\n"," 'Cervantes',\n"," 'Cetacea',\n"," 'Cetacean',\n"," 'Cetology',\n"," 'Cetus',\n"," 'Ceylon',\n"," 'Chace',\n"," 'Chaldee',\n"," 'Champagne',\n"," 'Champollion',\n"," 'Channel',\n"," 'Chapel',\n"," 'Charing',\n"," 'Charity',\n"," 'Charlemagne',\n"," 'Charley',\n"," 'Chart',\n"," 'Chartering',\n"," 'Chase',\n"," 'Cheever',\n"," 'Cherries',\n"," 'Chestnut',\n"," 'Chief',\n"," 'Childe',\n"," 'Chili',\n"," 'Chilian',\n"," 'China',\n"," 'Chinese',\n"," 'Cholo',\n"," 'Chowder',\n"," 'Christ',\n"," 'Christendom',\n"," 'Christian',\n"," 'Christianity',\n"," 'Christians',\n"," 'Christmas',\n"," 'Church',\n"," 'Cinque',\n"," 'Circassian',\n"," 'Circumambulate',\n"," 'Cistern',\n"," 'Civitas',\n"," 'Clam',\n"," 'Clap',\n"," 'Claus',\n"," 'Clay',\n"," 'Clear',\n"," 'Clearing',\n"," 'Cleopatra',\n"," 'Cleveland',\n"," 'Clifford',\n"," 'Clinging',\n"," 'Clootz',\n"," 'Close',\n"," 'Closing',\n"," 'Cloud',\n"," 'Cluny',\n"," 'Coast',\n"," 'Cock',\n"," 'Cockatoo',\n"," 'Cod',\n"," 'Cods',\n"," 'Coenties',\n"," 'Coffin',\n"," 'Coffins',\n"," 'Cognac',\n"," 'Coke',\n"," 'Cold',\n"," 'Coleman',\n"," 'Coleridge',\n"," 'College',\n"," 'Colnett',\n"," 'Cologne',\n"," 'Colonies',\n"," 'Colossus',\n"," 'Columbus',\n"," 'Come',\n"," 'Coming',\n"," 'Commanded',\n"," 'Commanders',\n"," 'Commend',\n"," 'Commodore',\n"," 'Commodores',\n"," 'Common',\n"," 'Commonly',\n"," 'Commons',\n"," 'Commonwealth',\n"," 'Companies',\n"," 'Comparing',\n"," 'Concerning',\n"," 'Congo',\n"," 'Congregation',\n"," 'Congregational',\n"," 'Conjuror',\n"," 'Connecticut',\n"," 'Consequently',\n"," 'Consider',\n"," 'Considering',\n"," 'Constable',\n"," 'Constantine',\n"," 'Constantinople',\n"," 'Consumptive',\n"," 'Continents',\n"," 'Contrasted',\n"," 'Conversation',\n"," 'Convulsively',\n"," 'Cook',\n"," 'Cooke',\n"," 'Cooks',\n"," 'Cooper',\n"," 'Coopman',\n"," 'Copenhagen',\n"," 'Coppered',\n"," 'Corinthians',\n"," 'Corkscrew',\n"," 'Corlaer',\n"," 'Corlears',\n"," 'Coronation',\n"," 'Corresponding',\n"," 'Corrupt',\n"," 'Cough',\n"," 'Could',\n"," 'Count',\n"," 'Counterpane',\n"," 'County',\n"," 'Court',\n"," 'Cousin',\n"," 'Cowper',\n"," 'Crab',\n"," 'Crack',\n"," 'Crammer',\n"," 'Crappo',\n"," 'Crappoes',\n"," 'Crazed',\n"," 'Creagh',\n"," 'Created',\n"," 'Cretan',\n"," 'Crete',\n"," 'Crew',\n"," 'Crish',\n"," 'Crockett',\n"," 'Cross',\n"," 'Crossed',\n"," 'Crossing',\n"," 'Crotch',\n"," 'Crowding',\n"," 'Crown',\n"," 'Crozetts',\n"," 'Cruelty',\n"," 'Cruising',\n"," 'Cruppered',\n"," 'Crusaders',\n"," 'Crushed',\n"," 'Crying',\n"," 'Cuba',\n"," 'Curious',\n"," 'Curse',\n"," 'Cursed',\n"," 'Curses',\n"," 'Cussed',\n"," 'Customs',\n"," 'Cut',\n"," 'Cutter',\n"," 'Cutting',\n"," 'Cuvier',\n"," 'Cyclades',\n"," 'Czar',\n"," 'D',\n"," 'Daboll',\n"," 'Daggoo',\n"," 'Dagon',\n"," 'Dame',\n"," 'Damn',\n"," 'Damocles',\n"," 'Dampier',\n"," 'Dan',\n"," 'Dance',\n"," 'Danes',\n"," 'Daniel',\n"," 'Danish',\n"," 'Dante',\n"," 'Dantean',\n"," 'Dar',\n"," 'Dardanelles',\n"," 'Darien',\n"," 'Darkness',\n"," 'Darmonodes',\n"," 'Dart',\n"," 'Dash',\n"," 'Dashing',\n"," 'Dauphine',\n"," 'Davis',\n"," 'Davy',\n"," 'Day',\n"," 'Days',\n"," 'De',\n"," 'Deacon',\n"," 'Dead',\n"," 'Death',\n"," 'Decanter',\n"," 'Decapitation',\n"," 'December',\n"," 'Deck',\n"," 'Deep',\n"," 'Deer',\n"," 'Deity',\n"," 'Del',\n"," 'Deliberately',\n"," 'Delight',\n"," 'Delightful',\n"," 'Deliverer',\n"," 'Delta',\n"," 'Den',\n"," 'Denderah',\n"," 'Depend',\n"," 'Derick',\n"," 'Dericks',\n"," 'Descartian',\n"," 'Descending',\n"," 'Desecrated',\n"," 'Desmarest',\n"," 'Desolation',\n"," 'Despairing',\n"," 'Despatch',\n"," 'Detached',\n"," 'Deuteronomy',\n"," 'Devil',\n"," 'Devils',\n"," 'Dey',\n"," 'Diaz',\n"," 'Dick',\n"," 'Did',\n"," 'Didn',\n"," 'Didst',\n"," 'Diminish',\n"," 'Ding',\n"," 'Dinner',\n"," 'Dinting',\n"," 'Discovery',\n"," 'Disdain',\n"," 'Dish',\n"," 'Dismal',\n"," 'Dissect',\n"," 'Dives',\n"," 'Divine',\n"," 'Diving',\n"," 'Do',\n"," 'Doctor',\n"," 'Dodge',\n"," 'Does',\n"," 'Doesn',\n"," 'Dog',\n"," 'Dolly',\n"," 'Dome',\n"," 'Dominic',\n"," 'Don',\n"," 'Dons',\n"," 'Doom',\n"," 'Dorchester',\n"," 'Dost',\n"," 'Doubloon',\n"," 'Doubtless',\n"," 'Doubts',\n"," 'Dough',\n"," 'Dover',\n"," 'Down',\n"," 'Dr',\n"," 'Dragged',\n"," 'Dragon',\n"," 'Drat',\n"," 'Drawing',\n"," 'Drawn',\n"," 'Draws',\n"," 'Drink',\n"," 'Drinking',\n"," 'Drive',\n"," 'Drop',\n"," 'Dropping',\n"," 'Dry',\n"," 'Duck',\n"," 'Dugongs',\n"," 'Duke',\n"," 'Dunder',\n"," 'Dunfermline',\n"," 'Dunkirk',\n"," 'Duodecimo',\n"," 'Duodecimoes',\n"," 'Durand',\n"," 'Durer',\n"," 'During',\n"," 'Dusk',\n"," 'Dut',\n"," 'Dutch',\n"," 'Dutchman',\n"," 'Dying',\n"," 'E',\n"," 'Each',\n"," 'Eagle',\n"," 'Earl',\n"," 'Earls',\n"," 'Earthsman',\n"," 'East',\n"," 'Eastern',\n"," 'Easy',\n"," 'Ebony',\n"," 'Ecclesiastes',\n"," 'Eckerman',\n"," 'Eddystone',\n"," 'Edgewise',\n"," 'Edmund',\n"," 'Edward',\n"," 'Ego',\n"," 'Egypt',\n"," 'Egyptian',\n"," 'Egyptians',\n"," 'Eh',\n"," 'Ehrenbreitstein',\n"," 'Eight',\n"," 'Either',\n"," 'Elbe',\n"," 'Electors',\n"," 'Elephant',\n"," 'Elephanta',\n"," 'Elephants',\n"," 'Elijah',\n"," 'Ellenborough',\n"," 'Elsewhere',\n"," 'Emblazonings',\n"," 'Emboldened',\n"," 'Emir',\n"," 'Emperor',\n"," 'Emperors',\n"," 'Empire',\n"," 'End',\n"," 'Enderbies',\n"," 'Enderby',\n"," 'Enderbys',\n"," 'England',\n"," 'Englander',\n"," 'English',\n"," 'Englishman',\n"," 'Englishmen',\n"," 'Enough',\n"," 'Enter',\n"," 'Entering',\n"," 'Entreaties',\n"," 'Enveloped',\n"," 'Ephesian',\n"," 'Epilogue',\n"," 'Epitome',\n"," 'Equality',\n"," 'Equator',\n"," 'Equatorial',\n"," 'Ere',\n"," 'Erie',\n"," 'Erromanggoans',\n"," 'Erroneous',\n"," 'Erskine',\n"," 'Esau',\n"," 'Espied',\n"," 'Espying',\n"," 'Esquimaux',\n"," 'Essex',\n"," 'Et',\n"," 'Eternities',\n"," 'Eternity',\n"," 'Ethiopian',\n"," 'Euclid',\n"," 'Euclidean',\n"," 'Euroclydon',\n"," 'Europa',\n"," 'Europe',\n"," 'European',\n"," 'Evangelist',\n"," 'Evangelists',\n"," 'Even',\n"," 'Ever',\n"," 'Every',\n"," 'Evil',\n"," 'Ex',\n"," 'Excellent',\n"," 'Excepting',\n"," 'Exception',\n"," 'Excuse',\n"," 'Expedition',\n"," 'Expeditions',\n"," 'Explain',\n"," 'Exploring',\n"," 'Extending',\n"," 'Ezekiel',\n"," 'F',\n"," 'Fa',\n"," 'Face',\n"," 'Fain',\n"," 'Faintly',\n"," 'Fair',\n"," 'Faith',\n"," 'Falsehood',\n"," 'Fanning',\n"," 'Far',\n"," 'Farewell',\n"," 'Fashioned',\n"," 'Fast',\n"," 'Fasting',\n"," 'Fat',\n"," 'Fata',\n"," 'Fate',\n"," 'Fates',\n"," 'Father',\n"," 'Fe',\n"," 'Fear',\n"," 'Fearing',\n"," 'February',\n"," 'Fedallah',\n"," 'Feegee',\n"," 'Feegeeans',\n"," 'Feegees',\n"," 'Feel',\n"," 'Feet',\n"," 'Fejee',\n"," 'Fellow',\n"," 'Ferdinando',\n"," 'Fernandes',\n"," 'Fetch',\n"," 'Few',\n"," 'Fields',\n"," 'Fiercely',\n"," 'Fiery',\n"," 'Fife',\n"," 'Fifth',\n"," 'Figuera',\n"," 'Fill',\n"," 'Fin',\n"," 'Finally',\n"," 'Find',\n"," 'Finding',\n"," 'Fine',\n"," 'Fired',\n"," 'First',\n"," 'Fish',\n"," 'Fisheries',\n"," 'Fishery',\n"," 'Fishes',\n"," 'Fishiest',\n"," 'Fits',\n"," 'Fitz',\n"," 'Five',\n"," 'Flask',\n"," 'Flat',\n"," 'Fleece',\n"," 'Fleet',\n"," 'Flip',\n"," 'Floating',\n"," 'Floundered',\n"," 'Flounders',\n"," 'Flukes',\n"," 'Flying',\n"," 'Fogo',\n"," 'Folding',\n"," 'Folger',\n"," 'Folgers',\n"," 'Folio',\n"," 'Folios',\n"," 'Fool',\n"," 'Foolish',\n"," 'For',\n"," 'Forced',\n"," 'Fore',\n"," 'Forecastle',\n"," 'Forehead',\n"," 'Foremost',\n"," 'Forge',\n"," 'Form',\n"," 'Forming',\n"," 'Formosa',\n"," 'Forthwith',\n"," 'Forty',\n"," 'Forward',\n"," 'Fossil',\n"," 'Fountain',\n"," 'Fourth',\n"," 'France',\n"," 'Frankfort',\n"," 'Franklin',\n"," 'Frederick',\n"," 'Free',\n"," 'Freely',\n"," 'Freeze',\n"," 'French',\n"," 'Frenchman',\n"," 'Frenchmen',\n"," 'Friar',\n"," 'Friend',\n"," 'Friends',\n"," 'Friesland',\n"," 'Frighted',\n"," 'Frobisher',\n"," 'Froissart',\n"," 'From',\n"," 'Fuego',\n"," 'Full',\n"," 'Funeral',\n"," 'Furl',\n"," 'Further',\n"," 'Furthermore',\n"," 'Future',\n"," 'Gabriel',\n"," 'Gaining',\n"," 'Gall',\n"," 'Galleries',\n"," 'Gallipagos',\n"," 'Gam',\n"," 'Gamming',\n"," 'Ganders',\n"," 'Ganges',\n"," 'Gardiner',\n"," 'Garnery',\n"," 'Gases',\n"," 'Gate',\n"," 'Gather',\n"," 'Gay',\n"," 'Gayer',\n"," 'Gayhead',\n"," 'Gazette',\n"," 'Gemini',\n"," 'General',\n"," 'Genesis',\n"," 'Geneva',\n"," 'Genius',\n"," 'Gentlemen',\n"," 'Gently',\n"," 'Geological',\n"," 'George',\n"," 'Ger',\n"," 'Germain',\n"," 'German',\n"," 'Germans',\n"," 'Gesner',\n"," 'Get',\n"," 'Ghent',\n"," 'Gibraltar',\n"," 'Gifted',\n"," 'Gilder',\n"," 'Ginger',\n"," 'Give',\n"," 'Giver',\n"," 'Giving',\n"," 'Glacier',\n"," 'Glancing',\n"," 'Glen',\n"," 'Gliding',\n"," 'Glimpses',\n"," 'Globe',\n"," 'Glory',\n"," 'Gnawed',\n"," 'Go',\n"," 'Goa',\n"," 'Goat',\n"," 'God',\n"," 'Gods',\n"," ...]"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"8YWHbsQyIhC4"},"source":["And there are more. if `wrd` is a string, then, for example:\n","\n","* `wrd.islower()` will return true if the word is all lowercase\n","* `wrd.isalpha()` will return true if all the character in the string are letters\n","\n","and there are also: `wrd.startswith('str')`, `wrd.isdigit()`, `wr.isalnum()`\n","and [many others](https://www.w3schools.com/python/python_ref_string.asp)."]},{"cell_type":"markdown","source":["## Exercise #4: Functions and substrings search"],"metadata":{"id":"jl2mxc9xa_nC"}},{"cell_type":"code","metadata":{"id":"wiP7qcUTIUS6"},"source":["from typing import List\n","\n","### Exercise:\n","\n","#def detect_string(tokens: List[str], search_str: str, search_position: int = 0) -> List[str]:\n","###Returns a sorted list of the vocabulary tokens which match the search conditions\n","\n","  #params:\n","   # tokens: a document tokens list.\n","   # search_str: a string to search in the token list\n","   # search_position: one of the following:\n","   #   0 - anywhere in the string\n","   #   1 - searches for the string at the beginning of the token\n","   #   2 - searches for the string at the end of the token\n","\n","  ### Fill in this function to returns the result of searching for the\n","  ### given string \"search_str\" in the token vocabulary \"tokens\", according to\n","  ### the position parameter, as explained in the docstring\n","\n","tokens = [word.lower() for word in text1]\n","\n","def detect_string(tokens: List[str], search_str: str, search_position: int = 0) -> List[str]:\n","    matching_tokens = set()\n","    for token in tokens:\n","        if search_position == 0:\n","          if search_str in token:\n","              matching_tokens.add(token)\n","        elif search_position == 1:\n","          if token.startswith(search_str):\n","              matching_tokens.add(token)\n","        elif search_position == 2:\n","          if token.endswith(search_str):\n","              matching_tokens.add(token)\n","        else:\n","          matching_tokens = []\n","    return sorted(matching_tokens)\n","\n","\n","###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6_t95CzIL27v"},"source":["### Test:\n","assert detect_string(text1, 'tably', 2) == ['comfortably',\n"," 'discreditably',\n"," 'illimitably',\n"," 'immutably',\n"," 'indubitably',\n"," 'inevitably',\n"," 'inscrutably',\n"," 'profitably',\n"," 'unaccountably',\n"," 'unwarrantably']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nrBjzeHIL4_V"},"source":["### Test:\n","assert detect_string(text1, 'argu', 1) == ['argue', 'argued', 'arguing', 'argument', 'arguments']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HOJPKwpnL6nQ"},"source":["### Test:\n","assert detect_string(text1, 'arg', 2) == []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n8V2lvmWL8TC"},"source":["### Test\n","assert detect_string(text1, 'larg') == ['enlarge',\n"," 'enlarged',\n"," 'enlarges',\n"," 'large',\n"," 'largely',\n"," 'largeness',\n"," 'larger',\n"," 'largest']"],"execution_count":null,"outputs":[]}]}